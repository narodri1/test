# Discuss similarities and differences in coding style

----

### Comments

**Example:**

|yours|LLM|
|---|---|
|<img width="1478" height="783" alt="image" src="https://github.com/user-attachments/assets/74177547-84e5-4851-8477-23b0a3801fec" />|<img width="877" height="262" alt="image" src="https://github.com/user-attachments/assets/162325b6-f55d-44e8-af69-addc4c5ddfad" />|

**Discussion:**

```
answer: our comment is more descriptive of the function and we should also note that we use inline comments (I also want to highlight that our comment is more of style that helps other when they read the code and ours isnt as formal) whereas the LLM ChatGPT's function description is much more short and to the point this is mainly cause LLM's like ChatGPT is trained on such a massive amount of data (typically for code its trained on high level code ie proffesor style code or maby even higher) and it also doesnt have any kinda inline comment. 
```

----

### Function declarations

**Example:**

|yours|LLM|
|---|---|
|<img width="1407" height="317" alt="image" src="https://github.com/user-attachments/assets/ef2623cc-0dd3-40d7-97f3-f92423cce96f" />|<img width="633" height="670" alt="image" src="https://github.com/user-attachments/assets/1d190b6a-55eb-4661-a5ba-87adcaea4926" />|

**Discussion:**

```
answer: our function declaration is much more simpler although it doesn't help the reader understand what the output we will get from the function while LLM's like ChatGPT makes the reader aware of what the function variables types are and what the output type is. This is mainly due to the fact that since the LLM's like ChatGPT is trained on huge amount of high level coding style in which its important that the reader is able to understand what the output of a function will look like without having to read through the whole function.
```

----

### Organization of the JSON file

**Example:**

|yours|LLM|
|---|---|
|<img width="699" height="385" alt="image" src="https://github.com/user-attachments/assets/d85763a9-0031-4c6c-86da-c1c60f3c4401" />|<img width="600" height="793" alt="image" src="https://github.com/user-attachments/assets/9333b63b-bac5-48e4-be3f-eaa14b34ab7a" />|

**Discussion:**

```
answer: we made our json file contain only things that are required and not any more whereas LLM's like ChatGPT contains much more description of things like it describes the BOS and EOS characters, the number of utterances, unigram totals and finally the length of the vocabulary. our code is more focused on the assignment and the task at hand so we didnt see the need to actually have unrequired information whereas since LLM's are trained on high level coding style ie more descriptive and able to adapt to things that might be added a later 
```

----

### Print statements

**Example:**

|yours|LLM|
|---|---|
|<img width="682" height="374" alt="image" src="https://github.com/user-attachments/assets/1c8819eb-693f-4904-b96d-47e65592e4d7" />|<img width="505" height="47" alt="image" src="https://github.com/user-attachments/assets/562c0d17-fdab-4922-9085-52df69d7872d" />|

**Discussion:**

```
answer: our print statements is much more simple and mainly to notify the user and reader that a specific process is done. our print statements are also much more informal  whereas compared to LLM's like ChatGPT thiers is much more direct and doesnt really show any care for the user by letting the user actually know where the program might be. This is because the when an LLM is trained it's on high level code so when ChatGPT generates print statement it follows that high level format.   
```

----

### Presenting the perplexity results

**Example:**

|yours|LLM|
|---|---|
|<img width="1135" height="116" alt="image" src="https://github.com/user-attachments/assets/d9ab5888-5828-4a89-abd4-1d05e23e811a" />|<img width="371" height="278" alt="image" src="https://github.com/user-attachments/assets/04e0d204-7025-4d9f-b062-a3a48d5a327e" />|

**Discussion:**

```
answer : again when we present the Perplexity result ours is more user friendly and prints a bit more info so that the user and reader understands which model, data type if the calculation is done using laplace smoothening and finally prints the perplexity ie this shows that we cares about the user and reader and thier understandability. while LLM's like chatgpt is much more focused on effiency that it only prints the perplexity values This is because an LLM like chatgpt is trained on such large amounts of data that use high level coding style in such a way that prioritizes the functionality of the code rather than the understandability of the reader. 
```
